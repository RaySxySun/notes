# Setup a kafka playground
> http://wurstmeister.github.io/kafka-docker/
> https://github.com/wurstmeister/kafka-docker

#================================= run tomcat docker
$ docker run -it --rm -p 8888:8080 hub.ecf-poc.com:9999/micro-service/fms-bff

#================================= busybox test
$ docker run -it  hub.ecf-poc.com:9999/library/busybox:latest ping trip-traffic-es-1

alias busybox="docker run -it --rm  busybox"

#================================= connectivity test
$ nc -vz -w 10 192.168.2.72 9200

#================================= docker-compose up doesn't pull down latest image if the image exists locally
$ docker-compose pull && docker-compose up

#================================= build docker images with specified network
$ docker run -it --network docker01 hub.ecf-poc.com:9999/library/fms-base:latest bash

#================================= merge files and filter out duplicate info (in Dockerfile)
COPY ./hosts /tmp/hosts
COPY ./fms-bff-latest/conf /opt/ecf/web_component/fms-bff-latest/conf
RUN cat /tmp/hosts /etc/hosts | sort -u > /etc/hosts

#================================= input & redirect
$ cat <<EOF > /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

#================================= k8s

kubectl create -f target/services/kube-dns.yaml
kubectl -n kube-system get svc

#================================= AWK
  # clean all invalid images
  docker rmi -f $(docker images | grep non | awk '{print $3}')
  # print 2nd & last column with space padding
  docker ps | grep fms | awk '{printf("%-40s %5s\n",$NF,$2)}'
  ```
  fms-report-engine                        hub.ecf-poc.com:9999/micro-service/fms-report-engine:latest
  fms-biz-rule-mgmt                        hub.ecf-poc.com:9999/micro-service/fms-biz-rule-mgmt:latest
  fms-org-mgmt                             hub.ecf-poc.com:9999/micro-service/fms-org-mgmt:latest
  fms-event-notification                   hub.ecf-poc.com:9999/micro-service/fms-event-notification:latest
  ```

#================================= Print curl pretty format
$ curl -XGET http://awesome.com/api/v1/data | python -m json.tool

#================================= kube-proxy
# ISSUE: error looking for path of conntrack: exec: "conntrack": executable file not found in $PATH
yum install conntrack-tools

#================================= check docker container
sudo docker inspect --format '{{ .State.Pid }}' reactor-worker


#================================= three-way&four-way handshake > establish connection
Client ------SYN-----> Server
Client <---ACK/SYN---- Server ----①
Client ------ACK-----> Server

> terminate connection
Client ------FIN-----> Server
Client <-----ACK------ Server ----②
Client <-----FIN------ Server ----③ (at this moment the client has been in FIN_WAIT_2 state)
Client ------ACK-----> Server

#================================= redis
# check all keys
> KEYS *


#================================= REACTOR

http://{{service_host}}:{{service_port}}/service/api/resourceBundle-management/v1/resourceBundle/deploy
ResourceBundleServiceRestful.deploy -> ResourceBundleServiceImpl.deploy (conent -> ApplicationArchive -> ResourceBundle -> save/update)
# READ file from postgres bytea field
$ psql -h 192.168.2.50 -p 5432 -U reactor -d reactor        #access postgres db
$ \d+ resourcebundle                                        #check table description

                              Table "public.resourcebundle"
   Column  |          Type          | Modifiers | Storage  | Stats target | Description 
  ---------+------------------------+-----------+----------+--------------+-------------
   id      | bigint                 | not null  | plain    |              | 
   content | oid                    | not null  | plain    |              | 
   name    | character varying(255) | not null  | extended |              | 
   version | character varying(255) | not null  | extended |              | 

$ select  * from resourcebundle;

  Id  Content Name                              Version
  ----------------------------------------------------------
  10  24578   reactor:resource-bundle:default   1
  40  16599   rms-resources-v2                  1.0-SNAPSHOT
  41  16600   ResourceBundle-Example            1.0-SNAPSHOT

  # -- read bytea 
  # -- $ \copy (SELECT encode(content, 'hex') FROM resourcebundle where name='ResourceBundle-Example') TO '/tmp/content.hex'
  # -- $ xxd -p -r content.hex > content.jpg
  # -- instead of reading file from bytea, the blob file should read directly with lo_get()

$ \copy (SELECT lo_get(content) FROM resourcebundle where name='reactor-app-ResourceBundle-1') TO '/tmp/content.hex';
$ xxd -p -r content.hex > content.jar

  # -- read bytea 
  # -- $ \copy (SELECT encode(content, 'hex') FROM resourcebundle where name='ResourceBundle-Example') TO '/tmp/content.hex'
  # -- $ xxd -p -r content.hex > content.jpg
  # -- instead of reading file from bytea, the blob file should read directly with lo_get()

  # -- 
  >> I know this is none of the user's business, but I'm wondering how
  >> blob's implemented as oid are stored in the postgresql data storage.
  >> Are each of them kept in a separate file, or are they chunked into
  >> pieces and scattered ?
  >
  > They're in chunks in the pg_largeobject catalog.
  > https://www.postgresql.org/docs/devel/catalog-pg-largeobject.html

#######################################################################################
#================================= HANDY CMD ==========================================
#######################################################################################
$> head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 	# generate 16 digits
$> date +%s 						# get second




#######################################################################################
#=============================== Trouble Shooting =====================================
#######################################################################################

# NullPointerException(NPE) will erase all important exception stack info 
# cause: jvm will optimize performance of NPE 
#        dubbo provider will return: RpcResult {"result":null,"exception":java.lang.NullPointerException(null)instance}
#        at the same time, Hessian2 will Deserialize Null 
#		            if (_constructor != null)
#		                return _constructor.newInstance(_constructorArgs);
#		            else
#		                return _type.newInstance();
#
```
java.lang.NullPointerException: null
	at sun.reflect.GeneratedConstructorAccessor81.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at com.alibaba.com.caucho.hessian.io.JavaDeserializer.instantiate(JavaDeserializer.java:275)
	at com.alibaba.com.caucho.hessian.io.JavaDeserializer.readObject(JavaDeserializer.java:159)
	at com.alibaba.com.caucho.hessian.io.SerializerFactory.readObject(SerializerFactory.java:420)
	at com.alibaba.com.caucho.hessian.io.Hessian2Input.readObjectInstance(Hessian2Input.java:2070)
	at com.alibaba.com.caucho.hessian.io.Hessian2Input.readObject(Hessian2Input.java:2005)
	at com.alibaba.com.caucho.hessian.io.Hessian2Input.readObject(Hessian2Input.java:1990)
	at com.alibaba.dubbo.common.serialize.support.hessian.Hessian2ObjectInput.readObject(Hessian2ObjectInput.java:88)
	at com.alibaba.dubbo.rpc.protocol.dubbo.DecodeableRpcResult.decode(DecodeableRpcResult.java:92)
	at com.alibaba.dubbo.rpc.protocol.dubbo.DecodeableRpcResult.decode(DecodeableRpcResult.java:109)
	at com.alibaba.dubbo.rpc.protocol.dubbo.DubboCodec.decodeBody(DubboCodec.java:97)
	at com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec.decode(ExchangeCodec.java:126)
	at com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec.decode(ExchangeCodec.java:87)
	at com.alibaba.dubbo.rpc.protocol.dubbo.DubboCountCodec.decode(DubboCountCodec.java:46)
	at com.alibaba.dubbo.remoting.transport.netty.NettyCodecAdapter$InternalDecoder.messageReceived(NettyCodecAdapter.java:134)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```
